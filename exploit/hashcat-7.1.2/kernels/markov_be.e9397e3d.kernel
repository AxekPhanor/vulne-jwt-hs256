//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: UNKNOWN
// Unknown Toolkit Version
// Based on NVVM 7.0.1
//

.version 9.0
.target sm_86, texmode_independent
.address_size 64

	// .globl	l_markov

.entry l_markov(
	.param .u64 .ptr .global .align 4 l_markov_param_0,
	.param .u64 .ptr .global .align 4 l_markov_param_1,
	.param .u64 .ptr .global .align 4 l_markov_param_2,
	.param .u64 l_markov_param_3,
	.param .u32 l_markov_param_4,
	.param .u32 l_markov_param_5,
	.param .u32 l_markov_param_6,
	.param .u32 l_markov_param_7,
	.param .u32 l_markov_param_8,
	.param .u64 l_markov_param_9
)
{
	.local .align 4 .b8 	__local_depot0[260];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .b32 	%r<129>;
	.reg .b64 	%rd<133>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd57, [l_markov_param_0];
	ld.param.u64 	%rd58, [l_markov_param_1];
	ld.param.u64 	%rd59, [l_markov_param_2];
	ld.param.u64 	%rd60, [l_markov_param_3];
	ld.param.u32 	%r24, [l_markov_param_4];
	ld.param.u32 	%r127, [l_markov_param_5];
	ld.param.u32 	%r26, [l_markov_param_6];
	ld.param.u32 	%r27, [l_markov_param_7];
	ld.param.u32 	%r28, [l_markov_param_8];
	ld.param.u64 	%rd61, [l_markov_param_9];
	add.u64 	%rd131, %SPL, 0;
	mov.b32 	%r29, %envreg3;
	cvt.s64.s32 	%rd63, %r29;
	mov.u32 	%r30, %ntid.x;
	mov.u32 	%r31, %ctaid.x;
	mul.wide.s32 	%rd64, %r30, %r31;
	mov.u32 	%r32, %tid.x;
	cvt.s64.s32 	%rd65, %r32;
	add.s64 	%rd66, %rd65, %rd63;
	add.s64 	%rd2, %rd66, %rd64;
	setp.ge.u64 	%p1, %rd2, %rd61;
	@%p1 bra 	$L__BB0_29;

	mov.u32 	%r33, 0;
	st.local.u32 	[%rd131], %r33;
	st.local.u32 	[%rd131+4], %r33;
	st.local.u32 	[%rd131+8], %r33;
	st.local.u32 	[%rd131+12], %r33;
	st.local.u32 	[%rd131+16], %r33;
	st.local.u32 	[%rd131+20], %r33;
	st.local.u32 	[%rd131+24], %r33;
	st.local.u32 	[%rd131+28], %r33;
	st.local.u32 	[%rd131+32], %r33;
	st.local.u32 	[%rd131+36], %r33;
	st.local.u32 	[%rd131+40], %r33;
	st.local.u32 	[%rd131+44], %r33;
	st.local.u32 	[%rd131+48], %r33;
	st.local.u32 	[%rd131+52], %r33;
	st.local.u32 	[%rd131+56], %r33;
	st.local.u32 	[%rd131+60], %r33;
	st.local.u32 	[%rd131+64], %r33;
	st.local.u32 	[%rd131+68], %r33;
	st.local.u32 	[%rd131+72], %r33;
	st.local.u32 	[%rd131+76], %r33;
	st.local.u32 	[%rd131+80], %r33;
	st.local.u32 	[%rd131+84], %r33;
	st.local.u32 	[%rd131+88], %r33;
	st.local.u32 	[%rd131+92], %r33;
	st.local.u32 	[%rd131+96], %r33;
	st.local.u32 	[%rd131+100], %r33;
	st.local.u32 	[%rd131+104], %r33;
	st.local.u32 	[%rd131+108], %r33;
	st.local.u32 	[%rd131+112], %r33;
	st.local.u32 	[%rd131+116], %r33;
	st.local.u32 	[%rd131+120], %r33;
	st.local.u32 	[%rd131+124], %r33;
	st.local.u32 	[%rd131+128], %r33;
	st.local.u32 	[%rd131+132], %r33;
	st.local.u32 	[%rd131+136], %r33;
	st.local.u32 	[%rd131+140], %r33;
	st.local.u32 	[%rd131+144], %r33;
	st.local.u32 	[%rd131+148], %r33;
	st.local.u32 	[%rd131+152], %r33;
	st.local.u32 	[%rd131+156], %r33;
	st.local.u32 	[%rd131+160], %r33;
	st.local.u32 	[%rd131+164], %r33;
	st.local.u32 	[%rd131+168], %r33;
	st.local.u32 	[%rd131+172], %r33;
	st.local.u32 	[%rd131+176], %r33;
	st.local.u32 	[%rd131+180], %r33;
	st.local.u32 	[%rd131+184], %r33;
	st.local.u32 	[%rd131+188], %r33;
	st.local.u32 	[%rd131+192], %r33;
	st.local.u32 	[%rd131+196], %r33;
	st.local.u32 	[%rd131+200], %r33;
	st.local.u32 	[%rd131+204], %r33;
	st.local.u32 	[%rd131+208], %r33;
	st.local.u32 	[%rd131+212], %r33;
	st.local.u32 	[%rd131+216], %r33;
	st.local.u32 	[%rd131+220], %r33;
	st.local.u32 	[%rd131+224], %r33;
	st.local.u32 	[%rd131+228], %r33;
	st.local.u32 	[%rd131+232], %r33;
	st.local.u32 	[%rd131+236], %r33;
	st.local.u32 	[%rd131+240], %r33;
	st.local.u32 	[%rd131+244], %r33;
	st.local.u32 	[%rd131+248], %r33;
	st.local.u32 	[%rd131+252], %r33;
	add.s32 	%r1, %r127, %r24;
	st.local.u32 	[%rd131+256], %r1;
	add.s64 	%rd116, %rd2, %rd60;
	setp.eq.s32 	%p2, %r24, 0;
	@%p2 bra 	$L__BB0_23;

	mul.wide.u32 	%rd67, %r127, 1028;
	add.s64 	%rd125, %rd58, %rd67;
	add.s32 	%r35, %r24, -1;
	and.b32  	%r126, %r24, 3;
	setp.lt.u32 	%p3, %r35, 3;
	@%p3 bra 	$L__BB0_17;

	sub.s32 	%r121, %r24, %r126;
	shl.b32 	%r36, %r127, 3;
	and.b32  	%r37, %r36, 24;
	xor.b32  	%r4, %r37, 24;
	add.s32 	%r38, %r36, 8;
	not.b32 	%r39, %r38;
	and.b32  	%r5, %r39, 24;
	xor.b32  	%r6, %r37, 8;
	add.s32 	%r40, %r36, -8;
	not.b32 	%r41, %r40;
	and.b32  	%r7, %r41, 24;

$L__BB0_4:
	ld.global.u32 	%rd8, [%rd125+1024];
	and.b64  	%rd68, %rd116, -4294967296;
	setp.eq.s64 	%p4, %rd68, 0;
	@%p4 bra 	$L__BB0_6;

	div.u64 	%rd117, %rd116, %rd8;
	mul.lo.s64 	%rd69, %rd117, %rd8;
	sub.s64 	%rd118, %rd116, %rd69;
	bra.uni 	$L__BB0_7;

$L__BB0_6:
	cvt.u32.u64 	%r42, %rd8;
	cvt.u32.u64 	%r43, %rd116;
	div.u32 	%r44, %r43, %r42;
	mul.lo.s32 	%r45, %r44, %r42;
	sub.s32 	%r46, %r43, %r45;
	cvt.u64.u32 	%rd117, %r44;
	cvt.u64.u32 	%rd118, %r46;

$L__BB0_7:
	shl.b64 	%rd70, %rd118, 2;
	add.s64 	%rd71, %rd125, %rd70;
	ld.global.u32 	%r47, [%rd71];
	shl.b32 	%r48, %r47, %r4;
	and.b32  	%r49, %r127, -4;
	cvt.u64.u32 	%rd72, %r49;
	add.s64 	%rd73, %rd131, %rd72;
	ld.local.u32 	%r50, [%rd73];
	or.b32  	%r51, %r50, %r48;
	st.local.u32 	[%rd73], %r51;
	shl.b32 	%r52, %r127, 8;
	add.s32 	%r53, %r47, %r52;
	cvt.u64.u32 	%rd15, %r53;
	mul.wide.u32 	%rd74, %r53, 1028;
	add.s64 	%rd75, %rd59, %rd74;
	ld.global.u32 	%rd16, [%rd75+1024];
	and.b64  	%rd76, %rd117, -4294967296;
	setp.eq.s64 	%p5, %rd76, 0;
	@%p5 bra 	$L__BB0_9;

	div.u64 	%rd119, %rd117, %rd16;
	mul.lo.s64 	%rd77, %rd119, %rd16;
	sub.s64 	%rd120, %rd117, %rd77;
	bra.uni 	$L__BB0_10;

$L__BB0_9:
	cvt.u32.u64 	%r54, %rd16;
	cvt.u32.u64 	%r55, %rd117;
	div.u32 	%r56, %r55, %r54;
	mul.lo.s32 	%r57, %r56, %r54;
	sub.s32 	%r58, %r55, %r57;
	cvt.u64.u32 	%rd119, %r56;
	cvt.u64.u32 	%rd120, %r58;

$L__BB0_10:
	add.s32 	%r59, %r127, 1;
	mul.lo.s64 	%rd78, %rd15, 1028;
	add.s64 	%rd79, %rd59, %rd78;
	shl.b64 	%rd80, %rd120, 2;
	add.s64 	%rd81, %rd79, %rd80;
	ld.global.u32 	%r60, [%rd81];
	shl.b32 	%r61, %r60, %r5;
	and.b32  	%r62, %r59, -4;
	cvt.u64.u32 	%rd82, %r62;
	add.s64 	%rd83, %rd131, %rd82;
	ld.local.u32 	%r63, [%rd83];
	or.b32  	%r64, %r63, %r61;
	st.local.u32 	[%rd83], %r64;
	shl.b32 	%r65, %r59, 8;
	add.s32 	%r66, %r60, %r65;
	cvt.u64.u32 	%rd23, %r66;
	mul.wide.u32 	%rd84, %r66, 1028;
	add.s64 	%rd85, %rd59, %rd84;
	ld.global.u32 	%rd24, [%rd85+1024];
	and.b64  	%rd86, %rd119, -4294967296;
	setp.eq.s64 	%p6, %rd86, 0;
	@%p6 bra 	$L__BB0_12;

	div.u64 	%rd121, %rd119, %rd24;
	mul.lo.s64 	%rd87, %rd121, %rd24;
	sub.s64 	%rd122, %rd119, %rd87;
	bra.uni 	$L__BB0_13;

$L__BB0_12:
	cvt.u32.u64 	%r67, %rd24;
	cvt.u32.u64 	%r68, %rd119;
	div.u32 	%r69, %r68, %r67;
	mul.lo.s32 	%r70, %r69, %r67;
	sub.s32 	%r71, %r68, %r70;
	cvt.u64.u32 	%rd121, %r69;
	cvt.u64.u32 	%rd122, %r71;

$L__BB0_13:
	add.s32 	%r72, %r127, 2;
	mul.lo.s64 	%rd88, %rd23, 1028;
	add.s64 	%rd89, %rd59, %rd88;
	shl.b64 	%rd90, %rd122, 2;
	add.s64 	%rd91, %rd89, %rd90;
	ld.global.u32 	%r73, [%rd91];
	shl.b32 	%r74, %r73, %r6;
	and.b32  	%r75, %r72, -4;
	cvt.u64.u32 	%rd92, %r75;
	add.s64 	%rd93, %rd131, %rd92;
	ld.local.u32 	%r76, [%rd93];
	or.b32  	%r77, %r76, %r74;
	st.local.u32 	[%rd93], %r77;
	shl.b32 	%r78, %r72, 8;
	add.s32 	%r79, %r73, %r78;
	cvt.u64.u32 	%rd31, %r79;
	mul.wide.u32 	%rd94, %r79, 1028;
	add.s64 	%rd95, %rd59, %rd94;
	ld.global.u32 	%rd32, [%rd95+1024];
	and.b64  	%rd96, %rd121, -4294967296;
	setp.eq.s64 	%p7, %rd96, 0;
	@%p7 bra 	$L__BB0_15;

	div.u64 	%rd116, %rd121, %rd32;
	mul.lo.s64 	%rd97, %rd116, %rd32;
	sub.s64 	%rd124, %rd121, %rd97;
	bra.uni 	$L__BB0_16;

$L__BB0_15:
	cvt.u32.u64 	%r80, %rd32;
	cvt.u32.u64 	%r81, %rd121;
	div.u32 	%r82, %r81, %r80;
	mul.lo.s32 	%r83, %r82, %r80;
	sub.s32 	%r84, %r81, %r83;
	cvt.u64.u32 	%rd116, %r82;
	cvt.u64.u32 	%rd124, %r84;

$L__BB0_16:
	add.s32 	%r85, %r127, 3;
	mul.lo.s64 	%rd98, %rd31, 1028;
	add.s64 	%rd99, %rd59, %rd98;
	shl.b64 	%rd100, %rd124, 2;
	add.s64 	%rd101, %rd99, %rd100;
	ld.global.u32 	%r86, [%rd101];
	shl.b32 	%r87, %r86, %r7;
	and.b32  	%r88, %r85, -4;
	cvt.u64.u32 	%rd102, %r88;
	add.s64 	%rd103, %rd131, %rd102;
	ld.local.u32 	%r89, [%rd103];
	or.b32  	%r90, %r89, %r87;
	st.local.u32 	[%rd103], %r90;
	shl.b32 	%r91, %r85, 8;
	add.s32 	%r92, %r86, %r91;
	mul.wide.u32 	%rd104, %r92, 1028;
	add.s64 	%rd125, %rd59, %rd104;
	add.s32 	%r127, %r127, 4;
	add.s32 	%r121, %r121, -4;
	setp.ne.s32 	%p8, %r121, 0;
	@%p8 bra 	$L__BB0_4;

$L__BB0_17:
	setp.eq.s32 	%p9, %r126, 0;
	@%p9 bra 	$L__BB0_23;

	shl.b32 	%r124, %r127, 8;

$L__BB0_19:
	.pragma "nounroll";
	ld.global.u32 	%rd44, [%rd125+1024];
	and.b64  	%rd105, %rd116, -4294967296;
	setp.eq.s64 	%p10, %rd105, 0;
	@%p10 bra 	$L__BB0_21;

	div.u64 	%rd45, %rd116, %rd44;
	mul.lo.s64 	%rd106, %rd45, %rd44;
	sub.s64 	%rd130, %rd116, %rd106;
	mov.u64 	%rd116, %rd45;
	bra.uni 	$L__BB0_22;

$L__BB0_21:
	cvt.u32.u64 	%r93, %rd44;
	cvt.u32.u64 	%r94, %rd116;
	div.u32 	%r95, %r94, %r93;
	mul.lo.s32 	%r96, %r95, %r93;
	sub.s32 	%r97, %r94, %r96;
	cvt.u64.u32 	%rd116, %r95;
	cvt.u64.u32 	%rd130, %r97;

$L__BB0_22:
	shl.b32 	%r98, %r127, 3;
	not.b32 	%r99, %r98;
	and.b32  	%r100, %r99, 24;
	shl.b64 	%rd107, %rd130, 2;
	add.s64 	%rd108, %rd125, %rd107;
	ld.global.u32 	%r101, [%rd108];
	shl.b32 	%r102, %r101, %r100;
	and.b32  	%r103, %r127, -4;
	cvt.u64.u32 	%rd109, %r103;
	add.s64 	%rd110, %rd131, %rd109;
	ld.local.u32 	%r104, [%rd110];
	or.b32  	%r105, %r104, %r102;
	st.local.u32 	[%rd110], %r105;
	add.s32 	%r106, %r124, %r101;
	mul.wide.u32 	%rd111, %r106, 1028;
	add.s64 	%rd125, %rd59, %rd111;
	add.s32 	%r127, %r127, 1;
	add.s32 	%r124, %r124, 256;
	add.s32 	%r126, %r126, -1;
	setp.ne.s32 	%p11, %r126, 0;
	@%p11 bra 	$L__BB0_19;

$L__BB0_23:
	shl.b32 	%r107, %r127, 3;
	not.b32 	%r108, %r107;
	and.b32  	%r109, %r108, 24;
	mov.u32 	%r110, 255;
	shl.b32 	%r111, %r110, %r109;
	and.b32  	%r112, %r111, %r26;
	and.b32  	%r113, %r127, -4;
	cvt.u64.u32 	%rd112, %r113;
	add.s64 	%rd113, %rd131, %rd112;
	ld.local.u32 	%r114, [%rd113];
	or.b32  	%r115, %r114, %r112;
	st.local.u32 	[%rd113], %r115;
	setp.eq.s32 	%p12, %r27, 0;
	@%p12 bra 	$L__BB0_25;

	shl.b32 	%r116, %r1, 3;
	st.local.u32 	[%rd131+56], %r116;

$L__BB0_25:
	setp.eq.s32 	%p13, %r28, 0;
	@%p13 bra 	$L__BB0_27;

	shl.b32 	%r117, %r1, 3;
	st.local.u32 	[%rd131+60], %r117;

$L__BB0_27:
	mul.lo.s64 	%rd114, %rd2, 260;
	add.s64 	%rd132, %rd57, %rd114;
	mov.u32 	%r128, 0;

$L__BB0_28:
	ld.local.u32 	%r119, [%rd131];
	st.global.u32 	[%rd132], %r119;
	add.s64 	%rd132, %rd132, 4;
	add.s64 	%rd131, %rd131, 4;
	add.s32 	%r128, %r128, 1;
	setp.lt.u32 	%p14, %r128, 65;
	@%p14 bra 	$L__BB0_28;

$L__BB0_29:
	ret;

}
	// .globl	r_markov
.entry r_markov(
	.param .u64 .ptr .global .align 4 r_markov_param_0,
	.param .u64 .ptr .global .align 4 r_markov_param_1,
	.param .u64 .ptr .global .align 4 r_markov_param_2,
	.param .u64 r_markov_param_3,
	.param .u32 r_markov_param_4,
	.param .u32 r_markov_param_5,
	.param .u32 r_markov_param_6,
	.param .u32 r_markov_param_7,
	.param .u64 r_markov_param_8
)
{
	.local .align 4 .b8 	__local_depot1[260];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<12>;
	.reg .b32 	%r<97>;
	.reg .b64 	%rd<124>;


	mov.u64 	%SPL, __local_depot1;
	ld.param.u64 	%rd50, [r_markov_param_0];
	ld.param.u64 	%rd118, [r_markov_param_1];
	ld.param.u64 	%rd52, [r_markov_param_2];
	ld.param.u64 	%rd53, [r_markov_param_3];
	ld.param.u32 	%r17, [r_markov_param_4];
	ld.param.u64 	%rd54, [r_markov_param_8];
	add.u64 	%rd1, %SPL, 0;
	mov.b32 	%r18, %envreg3;
	cvt.s64.s32 	%rd56, %r18;
	mov.u32 	%r19, %ntid.x;
	mov.u32 	%r20, %ctaid.x;
	mul.wide.s32 	%rd57, %r19, %r20;
	mov.u32 	%r21, %tid.x;
	cvt.s64.s32 	%rd58, %r21;
	add.s64 	%rd59, %rd58, %rd56;
	add.s64 	%rd2, %rd59, %rd57;
	setp.ge.u64 	%p1, %rd2, %rd54;
	@%p1 bra 	$L__BB1_24;

	mov.u32 	%r93, 0;
	st.local.u32 	[%rd1], %r93;
	st.local.u32 	[%rd1+4], %r93;
	st.local.u32 	[%rd1+8], %r93;
	st.local.u32 	[%rd1+12], %r93;
	st.local.u32 	[%rd1+16], %r93;
	st.local.u32 	[%rd1+20], %r93;
	st.local.u32 	[%rd1+24], %r93;
	st.local.u32 	[%rd1+28], %r93;
	st.local.u32 	[%rd1+32], %r93;
	st.local.u32 	[%rd1+36], %r93;
	st.local.u32 	[%rd1+40], %r93;
	st.local.u32 	[%rd1+44], %r93;
	st.local.u32 	[%rd1+48], %r93;
	st.local.u32 	[%rd1+52], %r93;
	st.local.u32 	[%rd1+56], %r93;
	st.local.u32 	[%rd1+60], %r93;
	st.local.u32 	[%rd1+64], %r93;
	st.local.u32 	[%rd1+68], %r93;
	st.local.u32 	[%rd1+72], %r93;
	st.local.u32 	[%rd1+76], %r93;
	st.local.u32 	[%rd1+80], %r93;
	st.local.u32 	[%rd1+84], %r93;
	st.local.u32 	[%rd1+88], %r93;
	st.local.u32 	[%rd1+92], %r93;
	st.local.u32 	[%rd1+96], %r93;
	st.local.u32 	[%rd1+100], %r93;
	st.local.u32 	[%rd1+104], %r93;
	st.local.u32 	[%rd1+108], %r93;
	st.local.u32 	[%rd1+112], %r93;
	st.local.u32 	[%rd1+116], %r93;
	st.local.u32 	[%rd1+120], %r93;
	st.local.u32 	[%rd1+124], %r93;
	st.local.u32 	[%rd1+128], %r93;
	st.local.u32 	[%rd1+132], %r93;
	st.local.u32 	[%rd1+136], %r93;
	st.local.u32 	[%rd1+140], %r93;
	st.local.u32 	[%rd1+144], %r93;
	st.local.u32 	[%rd1+148], %r93;
	st.local.u32 	[%rd1+152], %r93;
	st.local.u32 	[%rd1+156], %r93;
	st.local.u32 	[%rd1+160], %r93;
	st.local.u32 	[%rd1+164], %r93;
	st.local.u32 	[%rd1+168], %r93;
	st.local.u32 	[%rd1+172], %r93;
	st.local.u32 	[%rd1+176], %r93;
	st.local.u32 	[%rd1+180], %r93;
	st.local.u32 	[%rd1+184], %r93;
	st.local.u32 	[%rd1+188], %r93;
	st.local.u32 	[%rd1+192], %r93;
	st.local.u32 	[%rd1+196], %r93;
	st.local.u32 	[%rd1+200], %r93;
	st.local.u32 	[%rd1+204], %r93;
	st.local.u32 	[%rd1+208], %r93;
	st.local.u32 	[%rd1+212], %r93;
	st.local.u32 	[%rd1+216], %r93;
	st.local.u32 	[%rd1+220], %r93;
	st.local.u32 	[%rd1+224], %r93;
	st.local.u32 	[%rd1+228], %r93;
	st.local.u32 	[%rd1+232], %r93;
	st.local.u32 	[%rd1+236], %r93;
	st.local.u32 	[%rd1+240], %r93;
	st.local.u32 	[%rd1+244], %r93;
	st.local.u32 	[%rd1+248], %r93;
	st.local.u32 	[%rd1+252], %r93;
	add.s64 	%rd109, %rd2, %rd53;
	setp.eq.s32 	%p2, %r17, 0;
	@%p2 bra 	$L__BB1_23;

	add.s32 	%r24, %r17, -1;
	and.b32  	%r96, %r17, 3;
	setp.lt.u32 	%p3, %r24, 3;
	@%p3 bra 	$L__BB1_17;

	sub.s32 	%r92, %r17, %r96;
	mov.u32 	%r90, 0;
	mov.u32 	%r93, %r90;

$L__BB1_4:
	ld.global.u32 	%rd6, [%rd118+1024];
	and.b64  	%rd60, %rd109, -4294967296;
	setp.eq.s64 	%p4, %rd60, 0;
	@%p4 bra 	$L__BB1_6;

	div.u64 	%rd110, %rd109, %rd6;
	mul.lo.s64 	%rd61, %rd110, %rd6;
	sub.s64 	%rd111, %rd109, %rd61;
	bra.uni 	$L__BB1_7;

$L__BB1_6:
	cvt.u32.u64 	%r27, %rd6;
	cvt.u32.u64 	%r28, %rd109;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd110, %r29;
	cvt.u64.u32 	%rd111, %r31;

$L__BB1_7:
	shl.b64 	%rd62, %rd111, 2;
	add.s64 	%rd63, %rd118, %rd62;
	ld.global.u32 	%r32, [%rd63];
	shl.b32 	%r33, %r32, 24;
	cvt.u64.u32 	%rd64, %r93;
	add.s64 	%rd65, %rd1, %rd64;
	ld.local.u32 	%r34, [%rd65];
	or.b32  	%r35, %r34, %r33;
	st.local.u32 	[%rd65], %r35;
	add.s32 	%r36, %r90, %r32;
	cvt.u64.u32 	%rd13, %r36;
	mul.wide.u32 	%rd66, %r36, 1028;
	add.s64 	%rd67, %rd52, %rd66;
	ld.global.u32 	%rd14, [%rd67+1024];
	and.b64  	%rd68, %rd110, -4294967296;
	setp.eq.s64 	%p5, %rd68, 0;
	@%p5 bra 	$L__BB1_9;

	div.u64 	%rd112, %rd110, %rd14;
	mul.lo.s64 	%rd69, %rd112, %rd14;
	sub.s64 	%rd113, %rd110, %rd69;
	bra.uni 	$L__BB1_10;

$L__BB1_9:
	cvt.u32.u64 	%r37, %rd14;
	cvt.u32.u64 	%r38, %rd110;
	div.u32 	%r39, %r38, %r37;
	mul.lo.s32 	%r40, %r39, %r37;
	sub.s32 	%r41, %r38, %r40;
	cvt.u64.u32 	%rd112, %r39;
	cvt.u64.u32 	%rd113, %r41;

$L__BB1_10:
	mul.lo.s64 	%rd70, %rd13, 1028;
	add.s64 	%rd71, %rd52, %rd70;
	shl.b64 	%rd72, %rd113, 2;
	add.s64 	%rd73, %rd71, %rd72;
	ld.global.u32 	%r42, [%rd73];
	shl.b32 	%r43, %r42, 16;
	add.s32 	%r44, %r93, 1;
	and.b32  	%r45, %r44, -4;
	cvt.u64.u32 	%rd74, %r45;
	add.s64 	%rd75, %rd1, %rd74;
	ld.local.u32 	%r46, [%rd75];
	or.b32  	%r47, %r46, %r43;
	st.local.u32 	[%rd75], %r47;
	add.s32 	%r48, %r90, %r42;
	add.s32 	%r49, %r48, 256;
	cvt.u64.u32 	%rd21, %r49;
	mul.wide.u32 	%rd76, %r49, 1028;
	add.s64 	%rd77, %rd52, %rd76;
	ld.global.u32 	%rd22, [%rd77+1024];
	and.b64  	%rd78, %rd112, -4294967296;
	setp.eq.s64 	%p6, %rd78, 0;
	@%p6 bra 	$L__BB1_12;

	div.u64 	%rd114, %rd112, %rd22;
	mul.lo.s64 	%rd79, %rd114, %rd22;
	sub.s64 	%rd115, %rd112, %rd79;
	bra.uni 	$L__BB1_13;

$L__BB1_12:
	cvt.u32.u64 	%r50, %rd22;
	cvt.u32.u64 	%r51, %rd112;
	div.u32 	%r52, %r51, %r50;
	mul.lo.s32 	%r53, %r52, %r50;
	sub.s32 	%r54, %r51, %r53;
	cvt.u64.u32 	%rd114, %r52;
	cvt.u64.u32 	%rd115, %r54;

$L__BB1_13:
	mul.lo.s64 	%rd80, %rd21, 1028;
	add.s64 	%rd81, %rd52, %rd80;
	shl.b64 	%rd82, %rd115, 2;
	add.s64 	%rd83, %rd81, %rd82;
	ld.global.u32 	%r55, [%rd83];
	shl.b32 	%r56, %r55, 8;
	add.s32 	%r57, %r93, 2;
	and.b32  	%r58, %r57, -4;
	cvt.u64.u32 	%rd84, %r58;
	add.s64 	%rd85, %rd1, %rd84;
	ld.local.u32 	%r59, [%rd85];
	or.b32  	%r60, %r59, %r56;
	st.local.u32 	[%rd85], %r60;
	add.s32 	%r61, %r90, %r55;
	add.s32 	%r62, %r61, 512;
	cvt.u64.u32 	%rd29, %r62;
	mul.wide.u32 	%rd86, %r62, 1028;
	add.s64 	%rd87, %rd52, %rd86;
	ld.global.u32 	%rd30, [%rd87+1024];
	and.b64  	%rd88, %rd114, -4294967296;
	setp.eq.s64 	%p7, %rd88, 0;
	@%p7 bra 	$L__BB1_15;

	div.u64 	%rd109, %rd114, %rd30;
	mul.lo.s64 	%rd89, %rd109, %rd30;
	sub.s64 	%rd117, %rd114, %rd89;
	bra.uni 	$L__BB1_16;

$L__BB1_15:
	cvt.u32.u64 	%r63, %rd30;
	cvt.u32.u64 	%r64, %rd114;
	div.u32 	%r65, %r64, %r63;
	mul.lo.s32 	%r66, %r65, %r63;
	sub.s32 	%r67, %r64, %r66;
	cvt.u64.u32 	%rd109, %r65;
	cvt.u64.u32 	%rd117, %r67;

$L__BB1_16:
	add.s32 	%r68, %r93, 3;
	and.b32  	%r69, %r68, -4;
	cvt.u64.u32 	%rd90, %r69;
	add.s64 	%rd91, %rd1, %rd90;
	ld.local.u32 	%r70, [%rd91];
	mul.lo.s64 	%rd92, %rd29, 1028;
	add.s64 	%rd93, %rd52, %rd92;
	shl.b64 	%rd94, %rd117, 2;
	add.s64 	%rd95, %rd93, %rd94;
	ld.global.u32 	%r71, [%rd95];
	or.b32  	%r72, %r70, %r71;
	st.local.u32 	[%rd91], %r72;
	add.s32 	%r73, %r90, %r71;
	add.s32 	%r74, %r73, 768;
	mul.wide.u32 	%rd96, %r74, 1028;
	add.s64 	%rd118, %rd52, %rd96;
	add.s32 	%r93, %r93, 4;
	add.s32 	%r90, %r90, 1024;
	add.s32 	%r92, %r92, -4;
	setp.ne.s32 	%p8, %r92, 0;
	@%p8 bra 	$L__BB1_4;

$L__BB1_17:
	setp.eq.s32 	%p9, %r96, 0;
	@%p9 bra 	$L__BB1_23;

	shl.b32 	%r94, %r93, 8;

$L__BB1_19:
	.pragma "nounroll";
	ld.global.u32 	%rd42, [%rd118+1024];
	and.b64  	%rd97, %rd109, -4294967296;
	setp.eq.s64 	%p10, %rd97, 0;
	@%p10 bra 	$L__BB1_21;

	div.u64 	%rd43, %rd109, %rd42;
	mul.lo.s64 	%rd98, %rd43, %rd42;
	sub.s64 	%rd123, %rd109, %rd98;
	mov.u64 	%rd109, %rd43;
	bra.uni 	$L__BB1_22;

$L__BB1_21:
	cvt.u32.u64 	%r75, %rd42;
	cvt.u32.u64 	%r76, %rd109;
	div.u32 	%r77, %r76, %r75;
	mul.lo.s32 	%r78, %r77, %r75;
	sub.s32 	%r79, %r76, %r78;
	cvt.u64.u32 	%rd109, %r77;
	cvt.u64.u32 	%rd123, %r79;

$L__BB1_22:
	shl.b32 	%r80, %r93, 3;
	not.b32 	%r81, %r80;
	and.b32  	%r82, %r81, 24;
	shl.b64 	%rd99, %rd123, 2;
	add.s64 	%rd100, %rd118, %rd99;
	ld.global.u32 	%r83, [%rd100];
	shl.b32 	%r84, %r83, %r82;
	and.b32  	%r85, %r93, -4;
	cvt.u64.u32 	%rd101, %r85;
	add.s64 	%rd102, %rd1, %rd101;
	ld.local.u32 	%r86, [%rd102];
	or.b32  	%r87, %r86, %r84;
	st.local.u32 	[%rd102], %r87;
	add.s32 	%r88, %r94, %r83;
	mul.wide.u32 	%rd103, %r88, 1028;
	add.s64 	%rd118, %rd52, %rd103;
	add.s32 	%r93, %r93, 1;
	add.s32 	%r94, %r94, 256;
	add.s32 	%r96, %r96, -1;
	setp.ne.s32 	%p11, %r96, 0;
	@%p11 bra 	$L__BB1_19;

$L__BB1_23:
	ld.local.u32 	%r89, [%rd1];
	shl.b64 	%rd106, %rd2, 2;
	add.s64 	%rd107, %rd50, %rd106;
	st.global.u32 	[%rd107], %r89;

$L__BB1_24:
	ret;

}
	// .globl	C_markov
.entry C_markov(
	.param .u64 .ptr .global .align 4 C_markov_param_0,
	.param .u64 .ptr .global .align 4 C_markov_param_1,
	.param .u64 .ptr .global .align 4 C_markov_param_2,
	.param .u64 C_markov_param_3,
	.param .u32 C_markov_param_4,
	.param .u32 C_markov_param_5,
	.param .u32 C_markov_param_6,
	.param .u32 C_markov_param_7,
	.param .u64 C_markov_param_8
)
{
	.local .align 4 .b8 	__local_depot2[260];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .b32 	%r<122>;
	.reg .b64 	%rd<134>;


	mov.u64 	%SPL, __local_depot2;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd55, [C_markov_param_0];
	ld.param.u64 	%rd126, [C_markov_param_1];
	ld.param.u64 	%rd57, [C_markov_param_2];
	ld.param.u64 	%rd58, [C_markov_param_3];
	ld.param.u32 	%r21, [C_markov_param_4];
	ld.param.u32 	%r22, [C_markov_param_5];
	ld.param.u32 	%r23, [C_markov_param_6];
	ld.param.u32 	%r24, [C_markov_param_7];
	ld.param.u64 	%rd59, [C_markov_param_8];
	add.u64 	%rd60, %SP, 0;
	add.u64 	%rd132, %SPL, 0;
	mov.b32 	%r25, %envreg3;
	cvt.s64.s32 	%rd61, %r25;
	mov.u32 	%r26, %ntid.x;
	mov.u32 	%r27, %ctaid.x;
	mul.wide.s32 	%rd62, %r26, %r27;
	mov.u32 	%r28, %tid.x;
	cvt.s64.s32 	%rd63, %r28;
	add.s64 	%rd64, %rd63, %rd61;
	add.s64 	%rd2, %rd64, %rd62;
	setp.ge.u64 	%p1, %rd2, %rd59;
	@%p1 bra 	$L__BB2_29;

	mov.u32 	%r120, 0;
	st.local.u32 	[%rd132], %r120;
	st.local.u32 	[%rd132+4], %r120;
	st.local.u32 	[%rd132+8], %r120;
	st.local.u32 	[%rd132+12], %r120;
	st.local.u32 	[%rd132+16], %r120;
	st.local.u32 	[%rd132+20], %r120;
	st.local.u32 	[%rd132+24], %r120;
	st.local.u32 	[%rd132+28], %r120;
	st.local.u32 	[%rd132+32], %r120;
	st.local.u32 	[%rd132+36], %r120;
	st.local.u32 	[%rd132+40], %r120;
	st.local.u32 	[%rd132+44], %r120;
	st.local.u32 	[%rd132+48], %r120;
	st.local.u32 	[%rd132+52], %r120;
	st.local.u32 	[%rd132+56], %r120;
	st.local.u32 	[%rd132+60], %r120;
	st.local.u32 	[%rd132+64], %r120;
	st.local.u32 	[%rd132+68], %r120;
	st.local.u32 	[%rd132+72], %r120;
	st.local.u32 	[%rd132+76], %r120;
	st.local.u32 	[%rd132+80], %r120;
	st.local.u32 	[%rd132+84], %r120;
	st.local.u32 	[%rd132+88], %r120;
	st.local.u32 	[%rd132+92], %r120;
	st.local.u32 	[%rd132+96], %r120;
	st.local.u32 	[%rd132+100], %r120;
	st.local.u32 	[%rd132+104], %r120;
	st.local.u32 	[%rd132+108], %r120;
	st.local.u32 	[%rd132+112], %r120;
	st.local.u32 	[%rd132+116], %r120;
	st.local.u32 	[%rd132+120], %r120;
	st.local.u32 	[%rd132+124], %r120;
	st.local.u32 	[%rd132+128], %r120;
	st.local.u32 	[%rd132+132], %r120;
	st.local.u32 	[%rd132+136], %r120;
	st.local.u32 	[%rd132+140], %r120;
	st.local.u32 	[%rd132+144], %r120;
	st.local.u32 	[%rd132+148], %r120;
	st.local.u32 	[%rd132+152], %r120;
	st.local.u32 	[%rd132+156], %r120;
	st.local.u32 	[%rd132+160], %r120;
	st.local.u32 	[%rd132+164], %r120;
	st.local.u32 	[%rd132+168], %r120;
	st.local.u32 	[%rd132+172], %r120;
	st.local.u32 	[%rd132+176], %r120;
	st.local.u32 	[%rd132+180], %r120;
	st.local.u32 	[%rd132+184], %r120;
	st.local.u32 	[%rd132+188], %r120;
	st.local.u32 	[%rd132+192], %r120;
	st.local.u32 	[%rd132+196], %r120;
	st.local.u32 	[%rd132+200], %r120;
	st.local.u32 	[%rd132+204], %r120;
	st.local.u32 	[%rd132+208], %r120;
	st.local.u32 	[%rd132+212], %r120;
	st.local.u32 	[%rd132+216], %r120;
	st.local.u32 	[%rd132+220], %r120;
	st.local.u32 	[%rd132+224], %r120;
	st.local.u32 	[%rd132+228], %r120;
	st.local.u32 	[%rd132+232], %r120;
	st.local.u32 	[%rd132+236], %r120;
	st.local.u32 	[%rd132+240], %r120;
	st.local.u32 	[%rd132+244], %r120;
	st.local.u32 	[%rd132+248], %r120;
	st.local.u32 	[%rd132+252], %r120;
	st.local.u32 	[%rd132+256], %r21;
	add.s64 	%rd117, %rd2, %rd58;
	setp.eq.s32 	%p2, %r21, 0;
	@%p2 bra 	$L__BB2_23;

	add.s32 	%r32, %r21, -1;
	setp.lt.u32 	%p3, %r32, 3;
	mov.u32 	%r120, 0;
	@%p3 bra 	$L__BB2_17;

	and.b32  	%r35, %r21, 3;
	sub.s32 	%r114, %r21, %r35;
	mov.u32 	%r112, 0;
	mov.u32 	%r120, %r112;

$L__BB2_4:
	ld.global.u32 	%rd6, [%rd126+1024];
	and.b64  	%rd65, %rd117, -4294967296;
	setp.eq.s64 	%p4, %rd65, 0;
	@%p4 bra 	$L__BB2_6;

	div.u64 	%rd118, %rd117, %rd6;
	mul.lo.s64 	%rd66, %rd118, %rd6;
	sub.s64 	%rd119, %rd117, %rd66;
	bra.uni 	$L__BB2_7;

$L__BB2_6:
	cvt.u32.u64 	%r36, %rd6;
	cvt.u32.u64 	%r37, %rd117;
	div.u32 	%r38, %r37, %r36;
	mul.lo.s32 	%r39, %r38, %r36;
	sub.s32 	%r40, %r37, %r39;
	cvt.u64.u32 	%rd118, %r38;
	cvt.u64.u32 	%rd119, %r40;

$L__BB2_7:
	shl.b64 	%rd67, %rd119, 2;
	add.s64 	%rd68, %rd126, %rd67;
	ld.global.u32 	%r41, [%rd68];
	shl.b32 	%r42, %r41, 24;
	cvt.u64.u32 	%rd69, %r120;
	add.s64 	%rd70, %rd132, %rd69;
	ld.local.u32 	%r43, [%rd70];
	or.b32  	%r44, %r43, %r42;
	st.local.u32 	[%rd70], %r44;
	add.s32 	%r45, %r112, %r41;
	cvt.u64.u32 	%rd13, %r45;
	mul.wide.u32 	%rd71, %r45, 1028;
	add.s64 	%rd72, %rd57, %rd71;
	ld.global.u32 	%rd14, [%rd72+1024];
	and.b64  	%rd73, %rd118, -4294967296;
	setp.eq.s64 	%p5, %rd73, 0;
	@%p5 bra 	$L__BB2_9;

	div.u64 	%rd120, %rd118, %rd14;
	mul.lo.s64 	%rd74, %rd120, %rd14;
	sub.s64 	%rd121, %rd118, %rd74;
	bra.uni 	$L__BB2_10;

$L__BB2_9:
	cvt.u32.u64 	%r46, %rd14;
	cvt.u32.u64 	%r47, %rd118;
	div.u32 	%r48, %r47, %r46;
	mul.lo.s32 	%r49, %r48, %r46;
	sub.s32 	%r50, %r47, %r49;
	cvt.u64.u32 	%rd120, %r48;
	cvt.u64.u32 	%rd121, %r50;

$L__BB2_10:
	mul.lo.s64 	%rd75, %rd13, 1028;
	add.s64 	%rd76, %rd57, %rd75;
	shl.b64 	%rd77, %rd121, 2;
	add.s64 	%rd78, %rd76, %rd77;
	ld.global.u32 	%r51, [%rd78];
	shl.b32 	%r52, %r51, 16;
	add.s32 	%r53, %r120, 1;
	and.b32  	%r54, %r53, -4;
	cvt.u64.u32 	%rd79, %r54;
	add.s64 	%rd80, %rd132, %rd79;
	ld.local.u32 	%r55, [%rd80];
	or.b32  	%r56, %r55, %r52;
	st.local.u32 	[%rd80], %r56;
	add.s32 	%r57, %r112, %r51;
	add.s32 	%r58, %r57, 256;
	cvt.u64.u32 	%rd21, %r58;
	mul.wide.u32 	%rd81, %r58, 1028;
	add.s64 	%rd82, %rd57, %rd81;
	ld.global.u32 	%rd22, [%rd82+1024];
	and.b64  	%rd83, %rd120, -4294967296;
	setp.eq.s64 	%p6, %rd83, 0;
	@%p6 bra 	$L__BB2_12;

	div.u64 	%rd122, %rd120, %rd22;
	mul.lo.s64 	%rd84, %rd122, %rd22;
	sub.s64 	%rd123, %rd120, %rd84;
	bra.uni 	$L__BB2_13;

$L__BB2_12:
	cvt.u32.u64 	%r59, %rd22;
	cvt.u32.u64 	%r60, %rd120;
	div.u32 	%r61, %r60, %r59;
	mul.lo.s32 	%r62, %r61, %r59;
	sub.s32 	%r63, %r60, %r62;
	cvt.u64.u32 	%rd122, %r61;
	cvt.u64.u32 	%rd123, %r63;

$L__BB2_13:
	mul.lo.s64 	%rd85, %rd21, 1028;
	add.s64 	%rd86, %rd57, %rd85;
	shl.b64 	%rd87, %rd123, 2;
	add.s64 	%rd88, %rd86, %rd87;
	ld.global.u32 	%r64, [%rd88];
	shl.b32 	%r65, %r64, 8;
	add.s32 	%r66, %r120, 2;
	and.b32  	%r67, %r66, -4;
	cvt.u64.u32 	%rd89, %r67;
	add.s64 	%rd90, %rd132, %rd89;
	ld.local.u32 	%r68, [%rd90];
	or.b32  	%r69, %r68, %r65;
	st.local.u32 	[%rd90], %r69;
	add.s32 	%r70, %r112, %r64;
	add.s32 	%r71, %r70, 512;
	cvt.u64.u32 	%rd29, %r71;
	mul.wide.u32 	%rd91, %r71, 1028;
	add.s64 	%rd92, %rd57, %rd91;
	ld.global.u32 	%rd30, [%rd92+1024];
	and.b64  	%rd93, %rd122, -4294967296;
	setp.eq.s64 	%p7, %rd93, 0;
	@%p7 bra 	$L__BB2_15;

	div.u64 	%rd117, %rd122, %rd30;
	mul.lo.s64 	%rd94, %rd117, %rd30;
	sub.s64 	%rd125, %rd122, %rd94;
	bra.uni 	$L__BB2_16;

$L__BB2_15:
	cvt.u32.u64 	%r72, %rd30;
	cvt.u32.u64 	%r73, %rd122;
	div.u32 	%r74, %r73, %r72;
	mul.lo.s32 	%r75, %r74, %r72;
	sub.s32 	%r76, %r73, %r75;
	cvt.u64.u32 	%rd117, %r74;
	cvt.u64.u32 	%rd125, %r76;

$L__BB2_16:
	add.s32 	%r77, %r120, 3;
	and.b32  	%r78, %r77, -4;
	cvt.u64.u32 	%rd95, %r78;
	add.s64 	%rd96, %rd132, %rd95;
	ld.local.u32 	%r79, [%rd96];
	mul.lo.s64 	%rd97, %rd29, 1028;
	add.s64 	%rd98, %rd57, %rd97;
	shl.b64 	%rd99, %rd125, 2;
	add.s64 	%rd100, %rd98, %rd99;
	ld.global.u32 	%r80, [%rd100];
	or.b32  	%r81, %r79, %r80;
	st.local.u32 	[%rd96], %r81;
	add.s32 	%r82, %r112, %r80;
	add.s32 	%r83, %r82, 768;
	mul.wide.u32 	%rd101, %r83, 1028;
	add.s64 	%rd126, %rd57, %rd101;
	add.s32 	%r120, %r120, 4;
	add.s32 	%r112, %r112, 1024;
	add.s32 	%r114, %r114, -4;
	setp.ne.s32 	%p8, %r114, 0;
	@%p8 bra 	$L__BB2_4;

$L__BB2_17:
	and.b32  	%r119, %r21, 3;
	setp.eq.s32 	%p9, %r119, 0;
	@%p9 bra 	$L__BB2_23;

	shl.b32 	%r117, %r120, 8;

$L__BB2_19:
	.pragma "nounroll";
	ld.global.u32 	%rd42, [%rd126+1024];
	and.b64  	%rd102, %rd117, -4294967296;
	setp.eq.s64 	%p10, %rd102, 0;
	@%p10 bra 	$L__BB2_21;

	div.u64 	%rd43, %rd117, %rd42;
	mul.lo.s64 	%rd103, %rd43, %rd42;
	sub.s64 	%rd131, %rd117, %rd103;
	mov.u64 	%rd117, %rd43;
	bra.uni 	$L__BB2_22;

$L__BB2_21:
	cvt.u32.u64 	%r85, %rd42;
	cvt.u32.u64 	%r86, %rd117;
	div.u32 	%r87, %r86, %r85;
	mul.lo.s32 	%r88, %r87, %r85;
	sub.s32 	%r89, %r86, %r88;
	cvt.u64.u32 	%rd117, %r87;
	cvt.u64.u32 	%rd131, %r89;

$L__BB2_22:
	shl.b32 	%r90, %r120, 3;
	not.b32 	%r91, %r90;
	and.b32  	%r92, %r91, 24;
	shl.b64 	%rd104, %rd131, 2;
	add.s64 	%rd105, %rd126, %rd104;
	ld.global.u32 	%r93, [%rd105];
	shl.b32 	%r94, %r93, %r92;
	and.b32  	%r95, %r120, -4;
	cvt.u64.u32 	%rd106, %r95;
	add.s64 	%rd107, %rd132, %rd106;
	ld.local.u32 	%r96, [%rd107];
	or.b32  	%r97, %r96, %r94;
	st.local.u32 	[%rd107], %r97;
	add.s32 	%r98, %r117, %r93;
	mul.wide.u32 	%rd108, %r98, 1028;
	add.s64 	%rd126, %rd57, %rd108;
	add.s32 	%r120, %r120, 1;
	add.s32 	%r117, %r117, 256;
	add.s32 	%r119, %r119, -1;
	setp.ne.s32 	%p11, %r119, 0;
	@%p11 bra 	$L__BB2_19;

$L__BB2_23:
	shl.b32 	%r99, %r120, 3;
	not.b32 	%r100, %r99;
	and.b32  	%r101, %r100, 24;
	mov.u32 	%r102, 255;
	shl.b32 	%r103, %r102, %r101;
	and.b32  	%r104, %r103, %r22;
	and.b32  	%r105, %r120, -4;
	cvt.u64.u32 	%rd109, %r105;
	add.s64 	%rd110, %rd132, %rd109;
	ld.local.u32 	%r106, [%rd110];
	or.b32  	%r107, %r106, %r104;
	st.local.u32 	[%rd110], %r107;
	setp.eq.s32 	%p12, %r23, 0;
	@%p12 bra 	$L__BB2_25;

	shl.b32 	%r108, %r21, 3;
	cvta.to.local.u64 	%rd112, %rd60;
	st.local.u32 	[%rd112+56], %r108;

$L__BB2_25:
	setp.eq.s32 	%p13, %r24, 0;
	@%p13 bra 	$L__BB2_27;

	shl.b32 	%r109, %r21, 3;
	cvta.to.local.u64 	%rd114, %rd60;
	st.local.u32 	[%rd114+60], %r109;

$L__BB2_27:
	mul.lo.s64 	%rd115, %rd2, 260;
	add.s64 	%rd133, %rd55, %rd115;
	mov.u32 	%r121, 0;

$L__BB2_28:
	ld.local.u32 	%r111, [%rd132];
	st.global.u32 	[%rd133], %r111;
	add.s64 	%rd133, %rd133, 4;
	add.s64 	%rd132, %rd132, 4;
	add.s32 	%r121, %r121, 1;
	setp.lt.u32 	%p14, %r121, 65;
	@%p14 bra 	$L__BB2_28;

$L__BB2_29:
	ret;

}

  